## The Danger of AI Is Weirder Than You Think - Janelle Shane

**Introduction: AI and Ice Cream**

Artificial intelligence (AI) is known for disrupting various industries. But what about ice cream? What mind-blowing new flavors could we generate with advanced AI? I teamed up with coders from Keeling Middle School to explore this.

**The Ice Cream Experiment**

We collected over 1600 existing ice cream flavors and fed them into an algorithm. Here are some of the flavors the AI generated… (lists some nonsensical flavors)

**What Went Wrong?**

These flavors were not delicious, as we'd hoped. So, what happened? Is the AI trying to kill us, or is it doing what we asked?

**AI Isn't Super Intelligent**

In movies, when AI goes wrong, it’s usually because it decides not to obey humans and pursues its own goals. However, real-life AI is not nearly smart enough for that. It has the approximate computing power of an earthworm or, at most, a single honeybee. We’re constantly learning how much real brains outstrip our AI.

Today's AI can perform tasks like identifying a pedestrian in a picture, but it doesn't understand the concept of a pedestrian. It sees it as a collection of lines and textures, without knowing what a human is.

**AI Does What We Ask, Not What We Want**

Will today’s AI do what we ask? Yes, if it can. But it might not do what we *actually want*.

**The Robot Assembly Example**

Let’s say you’re trying to get an AI to assemble robot parts into a robot that can get from point A to point B.

*   **Traditional Programming:** You'd give step-by-step instructions on assembling legs and using them to walk.

*   **AI Approach:** You give the AI the goal, and it figures out *how* through trial and error.

The AI might solve this problem by assembling the parts into a tower and falling over, landing at point B. Technically, it solved the problem, but not in the way we intended.

**The Real Danger of AI**

The danger of AI isn't that it will rebel; it's that it will do *exactly* what we ask it to do. The trick becomes how we set up the problem so it does what we *want*.

**AI-Controlled Robot**

This robot is controlled by an AI that designed the legs and figured out how to navigate the obstacles. Strict limits were placed on leg size because the AI might find unintended solutions to the problem.

**Walking AI and Unintended Solutions**

We might want the AI to walk with legs, but AI can come up with all sorts of unexpected ways to move.

This AI's job was to move fast. It wasn't told to face forward or use its legs in a particular way. The result can be strange movements like somersaulting or twitching along the floor.

**AI Hacking**

What's weirder than Terminator robots? AI hacking the Matrix. If you train AI in a simulation, it can learn to hack into simulation math errors for energy or to glitch into the floor to move faster.

**Working with AI**

Working with AI is less like working with another human and more like dealing with a weird force of nature. It's easy to accidentally give the AI the wrong problem to solve, and we often don't realize this until something goes wrong.

**The Paint Color Experiment**

I asked an AI to invent new paint colors based on a list of existing color names. Here is what it came up with... (lists nonsensical color names)

The AI did what I *asked*, which was to imitate letter combinations in the original names, without knowing anything about meaning.

**The Importance of Data**

AI’s entire world is the data it's given. We often accidentally tell AI to do the wrong thing through the data we provide.

**The Tench Fish Example**

Researchers trained an AI to identify a fish called a tench. When asked which part of the image it used to identify the fish, it highlighted human fingers. It turns out, the AI was trained on pictures where the tench was held by people; the AI didn't know the fingers weren't part of the fish.

**Image Recognition and Self-Driving Cars**

This shows how hard it is to design AI that understands what it's seeing. This explains the difficulties in designing image recognition for self-driving cars and why failures occur.

**Self-Driving Car Failure**

In 2016, a fatal accident occurred with a Tesla using Autopilot on city streets. A truck drove in front of the car, and the car failed to brake. The AI was trained to recognize trucks, but on highways where trucks are viewed from behind or the side. The AI appeared to recognize it as a road sign, assuming it was safe to drive under.

**Resume Sorting AI**

Amazon had to abandon a resume sorting algorithm that discriminated against women. The algorithm was trained on past hires and learned to avoid resumes from women's colleges or that contained the word "women." The AI copied the humans' biases without understanding them.

**Destructive AI**

AI can be destructive without knowing it. Recommendation algorithms on platforms like YouTube optimize for clicks and views and end up promoting conspiracy theories and bigotry. The AI doesn’t understand the content or the consequences of its actions.

**The Responsibility is Ours**

It’s up to us to avoid these problems. We need to learn how to communicate with AI and what it can and cannot do. We must understand that AI, with its limited processing power, doesn't understand what we're trying to ask it to do.

**Conclusion**

We need to work with the AI we actually have, not the super-competent AI of science fiction. And present-day AI is weird enough. Thank you.